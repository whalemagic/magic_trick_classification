import pandas as pd
from tqdm import tqdm
import json
import logging
from hashlib import md5
from typing import Dict, List

from .categories import CATEGORIES, LABEL_TRANSLATIONS
from .classifier import load_model_and_tokenizer, batch_infer
from .rules import rule_based_tags
from .cache_utils import load_cache, save_cache
from .config import (
    DEFAULT_INPUT_PATH,
    DEFAULT_OUTPUT_PATH,
    DEFAULT_CACHE_PATH,
    DEFAULT_THRESHOLD,
    DEFAULT_TOP_K,
    DEFAULT_BATCH_SIZE,
    DEFAULT_RULES_PRIORITY
)

logger = logging.getLogger(__name__)

def translate_tags(tags: List[str]) -> List[str]:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–≥–∏ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫."""
    return [LABEL_TRANSLATIONS.get(tag, tag) for tag in tags]

def save_partial_results(df: pd.DataFrame, output_path: str, row_count: int):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    base, ext = os.path.splitext(output_path)
    partial_path = f"{base}.autosave_{row_count}{ext}"
    df.to_csv(partial_path, index=False)
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {partial_path}")

def generate_report(stats: dict) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    report = ["üìä –û—Ç—á—ë—Ç:"]
    report.append(f"‚úî –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_rows']} —Å—Ç—Ä–æ–∫")
    
    for cat in ['effect', 'props', 'scale', 'difficulty', 'product_type']:
        avg_tags = stats['tags_per_category'][cat] / stats['total_rows']
        report.append(f"   {cat:<12} ‚Äî –≤ —Å—Ä–µ–¥–Ω–µ–º {avg_tags:.1f} —Ç–µ–≥–∞(–æ–≤)/—Å—Ç—Ä–æ–∫—É")
    
    return "\n".join(report)

def tag_csv_with_progress(
    input_file=DEFAULT_INPUT_PATH,
    output_file=DEFAULT_OUTPUT_PATH,
    limit=None,
    threshold=DEFAULT_THRESHOLD,
    top_k=DEFAULT_TOP_K,
    batch_size=DEFAULT_BATCH_SIZE,
    cache_file=DEFAULT_CACHE_PATH,
    force=False,
    rules_priority=DEFAULT_RULES_PRIORITY
):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV —Ñ–∞–π–ª–∞."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à
        cache = load_cache(cache_file) if not force else {}

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞...")
        df = pd.read_csv(input_file)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {input_file}, —Å—Ç—Ä–æ–∫: {len(df)}")
        
        if limit:
            df = df.head(limit)
            logger.info(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {limit} —Å—Ç—Ä–æ–∫")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        classifier = load_model_and_tokenizer(batch_size)

        results = []
        all_json = []
        progress = tqdm(total=len(df), desc="Classifying", dynamic_ncols=True)

        # –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–Ω–µ—Ç –≤ –∫–µ—à–µ)
        to_process = []
        for i, row in df.iterrows():
            if limit and i >= limit:
                break
            desc = str(row.get("description", "")).strip()
            name = str(row.get("name", "")).strip()
            if not desc or desc.lower() == 'nan':
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ {i}: –ø—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
                continue
            h = md5(desc.encode()).hexdigest()
            if h not in cache:
                to_process.append((i, desc, name))
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(to_process)} —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–µ –≤ –∫–µ—à–µ)")

        # –ë–∞—Ç—á–∏–Ω–≥ –æ–ø–∏—Å–∞–Ω–∏–π
        for start in range(0, len(to_process), batch_size):
            batch = to_process[start:start+batch_size]
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {start//batch_size + 1}, —Ä–∞–∑–º–µ—Ä: {len(batch)}")
            
            batch_indices = [i for i, _, _ in batch]
            batch_descs = [desc for _, desc, _ in batch]
            batch_names = [name for _, _, name in batch]
            
            # –ü–æ–ª—É—á–∞–µ–º rule-based —Ç–µ–≥–∏
            rule_tags = [rule_based_tags(name, desc) for name, desc in zip(batch_names, batch_descs)]
            logger.debug(f"–ü–æ–ª—É—á–µ–Ω—ã rule-based —Ç–µ–≥–∏ –¥–ª—è –±–∞—Ç—á–∞: {rule_tags}")
            
            for cat, labels in CATEGORIES:
                cat_results = batch_infer(classifier, batch_descs, labels)
                for idx, res, rule_tag in zip(batch_indices, cat_results, rule_tags):
                    desc_val = str(df.loc[idx, "description"]).strip()
                    if not desc_val or desc_val.lower() == 'nan':
                        continue
                    h = md5(desc_val.encode()).hexdigest()
                    if h not in cache:
                        cache[h] = {}
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                    if rules_priority == 'prefer_rules' and rule_tag.get(cat):
                        cache[h][cat] = {tag: 1.0 for tag in rule_tag[cat]}
                    elif rules_priority == 'prefer_model':
                        cache[h][cat] = dict(zip(res["labels"], res["scores"]))
                    else:  # append
                        model_scores = dict(zip(res["labels"], res["scores"]))
                        rule_scores = {tag: 1.0 for tag in rule_tag.get(cat, [])}
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–≥–∏, –æ—Ç–¥–∞–≤–∞—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º
                        combined = {**model_scores, **rule_scores}
                        cache[h][cat] = combined
                        
            progress.update(len(batch))

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        tags_per_category = {cat: 0 for cat, _ in CATEGORIES}
        cache_hits = 0
        for i, row in df.iterrows():
            desc = str(row.get("description", "")).strip()
            name = str(row.get("name", "")).strip()
            if not desc or desc.lower() == 'nan':
                tag_json = {}
                all_json.append(json.dumps(tag_json, ensure_ascii=False))
                tags = {cat: [] for cat, _ in CATEGORIES}
                tags_ru = {cat: [] for cat, _ in CATEGORIES}
                results.append({**tags, **{cat+"_ru": tags_ru[cat] for cat in tags_ru}})
                continue
            h = md5(desc.encode()).hexdigest()
            tag_json = cache.get(h, {})
            if h in cache:
                cache_hits += 1
            all_json.append(json.dumps(tag_json, ensure_ascii=False))
            tags = {cat: [] for cat, _ in CATEGORIES}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            tags_ru = {cat: [] for cat, _ in CATEGORIES}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            for cat, labels in CATEGORIES:
                tag_scores = tag_json.get(cat, {})
                filtered = [label for label, score in sorted(tag_scores.items(), key=lambda x: -x[1]) if score >= threshold]
                if top_k:
                    filtered = filtered[:top_k]
                tags[cat] = filtered
                tags_ru[cat] = [LABEL_TRANSLATIONS.get(label, label) for label in filtered]
                tags_per_category[cat] += len(filtered)
            results.append({**tags, **{cat+"_ru": tags_ru[cat] for cat in tags_ru}})
        progress.close()

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
        for cat, _ in CATEGORIES:
            df[cat] = [r[cat] for r in results]
            df[cat+"_ru"] = [r[cat+"_ru"] for r in results]
        df["tags_json"] = all_json
        df.to_csv(output_file, index=False)
        print(f"‚úî Done! Saved to: {output_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à
        save_cache(cache, cache_file)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        total_rows = len(df)
        print("\nüìä –û—Ç—á—ë—Ç:")
        print(f"‚úî –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_rows} —Å—Ç—Ä–æ–∫")
        for cat in tags_per_category:
            avg = tags_per_category[cat] / total_rows if total_rows else 0
            print(f"   {cat:<12} ‚Äî –≤ —Å—Ä–µ–¥–Ω–µ–º {avg:.1f} —Ç–µ–≥–∞(–æ–≤)/—Å—Ç—Ä–æ–∫—É")
        print(f"‚úî –ö–µ—à-—Ö–∏—Ç–æ–≤: {cache_hits} –∏–∑ {total_rows}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        raise 