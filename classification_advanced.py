import pandas as pd
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification, ZeroShotClassificationPipeline
from hashlib import md5
import pickle
import json
import argparse
import os
import logging
from typing import Dict, List, Optional
from collections import defaultdict
import logging.handlers
from magic_classifier import (
    tag_csv_with_progress,
    DEFAULT_INPUT_PATH,
    DEFAULT_OUTPUT_PATH,
    DEFAULT_CACHE_PATH,
    DEFAULT_THRESHOLD,
    DEFAULT_TOP_K,
    DEFAULT_BATCH_SIZE,
    DEFAULT_RULES_PRIORITY
)
import time
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('classification.log', encoding='utf-8', mode='w')
    ]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
debug_handler = logging.FileHandler('debug.log', encoding='utf-8', mode='w')
debug_handler.setLevel(logging.DEBUG)
debug_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
debug_handler.setFormatter(debug_formatter)
logger.addHandler(debug_handler)

# –ü–µ—Ä–µ–≤–æ–¥—ã –º–µ—Ç–æ–∫
LABEL_TRANSLATIONS = {
    # –≠—Ñ—Ñ–µ–∫—Ç—ã
    "mentalism": "–º–µ–Ω—Ç–∞–ª–∏–∑–º",
    "prediction": "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ",
    "mind reading": "—á—Ç–µ–Ω–∏–µ –º—ã—Å–ª–µ–π",
    "mental revelation": "–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç–∫—Ä–æ–≤–µ–Ω–∏–µ",
    "vanish": "–∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ",
    "appearance": "–ø–æ—è–≤–ª–µ–Ω–∏–µ",
    "transformation": "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è",
    "restoration": "–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ",
    "levitation": "–ª–µ–≤–∏—Ç–∞—Ü–∏—è",
    "penetration": "–ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ",
    "transposition": "—Ç—Ä–∞–Ω—Å–ø–æ–∑–∏—Ü–∏—è",
    "teleportation": "—Ç–µ–ª–µ–ø–æ—Ä—Ç–∞—Ü–∏—è",
    "card revelation": "–æ—Ç–∫—Ä–æ–≤–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã",
    "card control": "–∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—Ä—Ç—ã",
    "card location": "–ª–æ–∫–∞—Ü–∏—è –∫–∞—Ä—Ç—ã",
    "coin magic": "—Ñ–æ–∫—É—Å—ã —Å –º–æ–Ω–µ—Ç–∞–º–∏",
    "coin vanish": "–∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –º–æ–Ω–µ—Ç—ã",
    "coin production": "–ø–æ—è–≤–ª–µ–Ω–∏–µ –º–æ–Ω–µ—Ç—ã",
    
    # –†–µ–∫–≤–∏–∑–∏—Ç
    "cards": "–∫–∞—Ä—Ç—ã",
    "coins": "–º–æ–Ω–µ—Ç—ã",
    "phone": "—Ç–µ–ª–µ—Ñ–æ–Ω",
    "paper": "–±—É–º–∞–≥–∞",
    "bills": "–∫—É–ø—é—Ä—ã",
    "magic wand": "–≤–æ–ª—à–µ–±–Ω–∞—è –ø–∞–ª–æ—á–∫–∞",
    "magic box": "–≤–æ–ª—à–µ–±–Ω–∞—è –∫–æ—Ä–æ–±–∫–∞",
    "magic apparatus": "–º–∞–≥–∏—á–µ—Å–∫–∏–π –∞–ø–ø–∞—Ä–∞—Ç",
    "gimmick": "–≥–∏–º–º–∏–∫",
    "prop": "—Ä–µ–∫–≤–∏–∑–∏—Ç",
    "device": "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ",
    
    # –ú–∞—Å—à—Ç–∞–± –∏ —Å—Ç–∏–ª—å
    "close-up magic": "–º–∏–∫—Ä–æ–º–∞–≥–∏—è",
    "parlour magic": "—Å–∞–ª–æ–Ω–Ω–∞—è –º–∞–≥–∏—è",
    "stage magic": "—Å—Ü–µ–Ω–∏—á–µ—Å–∫–∞—è –º–∞–≥–∏—è",
    "comedy magic": "–∫–æ–º–µ–¥–∏–π–Ω–∞—è –º–∞–≥–∏—è",
    "serious magic": "—Å–µ—Ä—å—ë–∑–Ω–∞—è –º–∞–≥–∏—è",
    "interactive magic": "–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –º–∞–≥–∏—è",
    
    # –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    "beginner": "–Ω–∞—á–∏–Ω–∞—é—â–∏–π",
    "intermediate": "—Å—Ä–µ–¥–Ω–∏–π",
    "advanced": "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π",
    "professional": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π",
    "easy to learn": "–ª–µ–≥–∫–æ —É—á–∏—Ç—Å—è",
    "requires practice": "—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–∞–∫—Ç–∏–∫–∏",
    "requires skill": "—Ç—Ä–µ–±—É–µ—Ç –Ω–∞–≤—ã–∫–∞",
    
    # –¢–∏–ø —Ç–æ–≤–∞—Ä–∞
    "physical product": "—Ñ–∏–∑–∏—á–µ—Å–∫–∏–π —Ç–æ–≤–∞—Ä",
    "digital download": "—Ü–∏—Ñ—Ä–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞",
    "video": "–≤–∏–¥–µ–æ",
    "book": "–∫–Ω–∏–≥–∞",
    "magazine": "–∂—É—Ä–Ω–∞–ª",
    "kit": "–Ω–∞–±–æ—Ä"
}

# 1. –û—Å–Ω–æ–≤–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
effects_eng = [
    "mentalism", "prediction", "mind reading", "mental revelation",
    "vanish", "appearance", "transformation", "restoration",
    "levitation", "penetration", "transposition", "teleportation",
    "card revelation", "card control", "card location",
    "coin magic", "coin vanish", "coin production"
]

# 2. –†–µ–∫–≤–∏–∑–∏—Ç
props_eng = [
    "cards", "coins", "phone", "paper", "bills",
    "magic wand", "magic box", "magic apparatus",
    "gimmick", "prop", "device"
]

# 3. –ú–∞—Å—à—Ç–∞–± –∏ —Å—Ç–∏–ª—å
scale_eng = [
    "close-up magic", "parlour magic", "stage magic",
    "comedy magic", "serious magic", "interactive magic"
]

# 4. –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
difficulty_eng = [
    "beginner", "intermediate", "advanced", "professional",
    "easy to learn", "requires practice", "requires skill"
]

# 5. –¢–∏–ø —Ç–æ–≤–∞—Ä–∞
product_type_eng = [
    "physical product", "digital download", "video",
    "book", "magazine", "kit"
]

def rule_based_tags(name: str, description: str) -> Dict[str, List[str]]:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è."""
    logger.debug(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è: {name}")
    tags = defaultdict(list)
    
    # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ç–∏–ø–∞ —Ç–æ–≤–∞—Ä–∞
    if any(x in name.lower() for x in ["instant download", "digital download", "download"]):
        tags["product_type"].extend(["digital download", "video"])
        logger.debug(f"–ù–∞–π–¥–µ–Ω —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞: digital download, video")
    if any(x in name.lower() for x in ["dvd", "video", "tutorial", "streaming"]):
        tags["product_type"].append("video")
    if any(x in name.lower() for x in ["book", "manual", "guide", "pdf", "ebook"]):
        tags["product_type"].append("book")
    if any(x in name.lower() for x in ["magazine", "journal", "periodical"]):
        tags["product_type"].append("magazine")
    if any(x in name.lower() for x in ["kit", "set", "bundle", "collection"]):
        tags["product_type"].append("kit")
    if not tags["product_type"]:
        tags["product_type"].append("physical product")
    
    # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    if any(x in name.lower() + description.lower() for x in ["beginner", "easy", "basic", "starter", "novice"]):
        tags["difficulty"].extend(["beginner", "easy to learn"])
    if any(x in name.lower() + description.lower() for x in ["intermediate", "medium", "moderate"]):
        tags["difficulty"].append("intermediate")
    if any(x in name.lower() + description.lower() for x in ["advanced", "expert", "complex", "difficult"]):
        tags["difficulty"].append("advanced")
    if any(x in name.lower() + description.lower() for x in ["professional", "pro", "master"]):
        tags["difficulty"].append("professional")
    
    # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä–µ–∫–≤–∏–∑–∏—Ç–∞
    if any(x in name.lower() + description.lower() for x in ["card", "deck", "playing cards", "bicycle"]):
        tags["props"].append("cards")
    if any(x in name.lower() + description.lower() for x in ["coin", "coins", "dollar", "quarter"]):
        tags["props"].append("coins")
    if any(x in name.lower() + description.lower() for x in ["phone", "smartphone", "mobile", "iphone", "android"]):
        tags["props"].append("phone")
    if any(x in name.lower() + description.lower() for x in ["paper", "bill", "note", "money"]):
        tags["props"].append("paper")
    if any(x in name.lower() + description.lower() for x in ["wand", "stick", "magic wand"]):
        tags["props"].append("magic wand")
    if any(x in name.lower() + description.lower() for x in ["box", "case", "container"]):
        tags["props"].append("magic box")
    if any(x in name.lower() + description.lower() for x in ["gimmick", "gimmicked", "special prop"]):
        tags["props"].append("gimmick")
    if any(x in name.lower() + description.lower() for x in ["apparatus", "device", "equipment", "machine"]):
        tags["props"].append("magic apparatus")
    
    # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∞
    if any(x in name.lower() + description.lower() for x in ["close-up", "closeup", "table", "intimate"]):
        tags["scale"].append("close-up magic")
    if any(x in name.lower() + description.lower() for x in ["parlour", "parlor", "salon", "living room"]):
        tags["scale"].append("parlour magic")
    if any(x in name.lower() + description.lower() for x in ["stage", "show", "performance", "theater"]):
        tags["scale"].append("stage magic")
    if any(x in name.lower() + description.lower() for x in ["comedy", "funny", "humor", "entertaining"]):
        tags["scale"].append("comedy magic")
    if any(x in name.lower() + description.lower() for x in ["serious", "professional", "dramatic"]):
        tags["scale"].append("serious magic")
    if any(x in name.lower() + description.lower() for x in ["interactive", "audience", "participation", "spectator"]):
        tags["scale"].append("interactive magic")
    
    # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
    if any(x in name.lower() + description.lower() for x in ["mental", "mind", "psychic", "psychological"]):
        tags["effect"].append("mentalism")
    if any(x in name.lower() + description.lower() for x in ["predict", "prediction", "forecast", "foretell"]):
        tags["effect"].append("prediction")
    if any(x in name.lower() + description.lower() for x in ["mind reading", "telepathy", "thought"]):
        tags["effect"].append("mind reading")
    if any(x in name.lower() + description.lower() for x in ["vanish", "disappear", "vanishing", "invisible"]):
        tags["effect"].append("vanish")
    if any(x in name.lower() + description.lower() for x in ["appear", "production", "produce", "materialize"]):
        tags["effect"].append("appearance")
    if any(x in name.lower() + description.lower() for x in ["transform", "change", "morph", "transmutation"]):
        tags["effect"].append("transformation")
    if any(x in name.lower() + description.lower() for x in ["restore", "restoration", "repair", "fix"]):
        tags["effect"].append("restoration")
    if any(x in name.lower() + description.lower() for x in ["levitate", "levitation", "float", "suspend"]):
        tags["effect"].append("levitation")
    if any(x in name.lower() + description.lower() for x in ["penetrate", "penetration", "through", "pass through"]):
        tags["effect"].append("penetration")
    if any(x in name.lower() + description.lower() for x in ["transpose", "transposition", "switch", "exchange"]):
        tags["effect"].append("transposition")
    
    return dict(tags)

def get_args():
    parser = argparse.ArgumentParser(description='–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è CSV —Ñ–∞–π–ª–∞ —Å —Ç–µ–≥–∞–º–∏')
    parser.add_argument('--input', type=str, default=DEFAULT_INPUT_PATH, help='–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É')
    parser.add_argument('--output', type=str, default=DEFAULT_OUTPUT_PATH, help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É')
    parser.add_argument('--limit', type=int, default=30, help='–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--threshold', type=float, default=DEFAULT_THRESHOLD, help='–ü–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–≥–æ–≤')
    parser.add_argument('--top_k', type=int, default=DEFAULT_TOP_K, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
    parser.add_argument('--batch_size', type=int, default=DEFAULT_BATCH_SIZE, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--cache', type=str, default=DEFAULT_CACHE_PATH, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞')
    parser.add_argument('--force', action='store_true', help='–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à')
    parser.add_argument('--rules_priority', type=str, default=DEFAULT_RULES_PRIORITY, 
                      choices=['append', 'prefer_rules', 'prefer_model'],
                      help='–ö–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å rule-based —Ç–µ–≥–∏ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏')
    return parser.parse_args()

def limit_tags_json(tag_scores: dict, max_tags: int = 10) -> dict:
    """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ –≤ tags_json."""
    all_scores = []
    for cat_scores in tag_scores.values():
        all_scores.extend(cat_scores.items())
    
    return dict(sorted(all_scores, key=lambda x: -x[1])[:max_tags])

def translate_tags(tags: List[str]) -> List[str]:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–≥–∏ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫."""
    return [LABEL_TRANSLATIONS.get(tag, tag) for tag in tags]

def save_partial_results(df: pd.DataFrame, output_path: str, row_count: int):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    base, ext = os.path.splitext(output_path)
    partial_path = f"{base}.autosave_{row_count}{ext}"
    df.to_csv(partial_path, index=False)
    logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {partial_path}")

def generate_report(stats: dict) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
    report = []
    report.append("=== –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ===\n")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    report.append("–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    report.append(f"- –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {stats.get('total_items', 0)}")
    report.append(f"- –£—Å–ø–µ—à–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {stats.get('successful_items', 0)}")
    report.append(f"- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: {stats.get('cached_items', 0)} —Ä–∞–∑")
    report.append(f"- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ç–æ–≤–∞—Ä: {stats.get('avg_time_per_item', 0):.2f} —Å–µ–∫")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    if 'category_stats' in stats:
        report.append("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, count in stats['category_stats'].items():
            report.append(f"- {LABEL_TRANSLATIONS.get(category, category)}: {count} —Ç–µ–≥–æ–≤")
    
    # –¢–æ–ø —Ç–µ–≥–æ–≤
    if 'top_tags' in stats:
        report.append("\n–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–µ–≥–∏:")
        for tag, count in sorted(stats['top_tags'].items(), key=lambda x: x[1], reverse=True)[:10]:
            report.append(f"- {LABEL_TRANSLATIONS.get(tag, tag)}: {count} —Ä–∞–∑")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º —Ç–µ–≥–æ–≤
    if 'tag_sources' in stats:
        report.append("\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–µ–≥–æ–≤:")
        report.append(f"- –ü—Ä–∞–≤–∏–ª–∞: {stats['tag_sources'].get('rules', 0)}")
        report.append(f"- –ú–æ–¥–µ–ª—å: {stats['tag_sources'].get('model', 0)}")
        report.append(f"- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {stats['tag_sources'].get('combined', 0)}")
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∏ –æ—à–∏–±–∫–∏
    if stats.get('warnings', []):
        report.append("\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for warning in stats['warnings']:
            report.append(f"- {warning}")
    
    if stats.get('errors', []):
        report.append("\n–û—à–∏–±–∫–∏:")
        for error in stats['errors']:
            report.append(f"- {error}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    report.append("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    if stats.get('low_confidence_items', 0) > 0:
        report.append("- –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–æ–≤–∞—Ä—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        report.append(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {stats['low_confidence_items']}")
        report.append("  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —ç—Ç–∏ —Ç–æ–≤–∞—Ä—ã –≤—Ä—É—á–Ω—É—é")
    
    if stats.get('missing_categories', []):
        report.append("- –°–ª–µ–¥—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö:")
        for category in stats['missing_categories']:
            report.append(f"  * {LABEL_TRANSLATIONS.get(category, category)}")
        report.append("  –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    
    return "\n".join(report)

def load_cache(cache_path):
    if os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    return {}

def save_cache(cache, cache_path):
    with open(cache_path, 'wb') as f:
        pickle.dump(cache, f)

def batch_infer(classifier, descs, labels):
    # descs: —Å–ø–∏—Å–æ–∫ –æ–ø–∏—Å–∞–Ω–∏–π, labels: —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫
    results = classifier(descs, labels, multi_label=True)
    # results ‚Äî —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (–∏–ª–∏ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å, –µ—Å–ª–∏ 1 –æ–ø–∏—Å–∞–Ω–∏–µ)
    if isinstance(results, dict):
        results = [results]
    return results

def collect_statistics(df: pd.DataFrame, cache: dict, start_time: float) -> dict:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –æ—Ç—á–µ—Ç–∞."""
    stats = {
        'total_items': len(df),
        'successful_items': 0,
        'cached_items': 0,
        'avg_time_per_item': 0,
        'category_stats': defaultdict(int),
        'top_tags': defaultdict(int),
        'tag_sources': {'rules': 0, 'model': 0, 'combined': 0},
        'warnings': [],
        'errors': [],
        'low_confidence_items': 0,
        'missing_categories': []
    }
    
    # –ü–æ–¥—Å—á–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    for _, row in df.iterrows():
        desc = str(row.get("description", "")).strip()
        if not desc or desc.lower() == 'nan':
            continue
        h = md5(desc.encode()).hexdigest()
        if h in cache:
            stats['cached_items'] += 1
            cache_data = cache[h]
            
            # –ü–æ–¥—Å—á–µ—Ç —Ç–µ–≥–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            for category, tags in cache_data.items():
                stats['category_stats'][category] += len(tags)
                for tag, confidence in tags.items():
                    stats['top_tags'][tag] += 1
                    if confidence < 0.3:  # –ü–æ—Ä–æ–≥ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                        stats['low_confidence_items'] += 1
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç–µ–≥–æ–≤
            for category, tags in cache_data.items():
                if all(score == 1.0 for score in tags.values()):
                    stats['tag_sources']['rules'] += 1
                elif all(score < 1.0 for score in tags.values()):
                    stats['tag_sources']['model'] += 1
                else:
                    stats['tag_sources']['combined'] += 1
            
            stats['successful_items'] += 1
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    all_categories = set(['effect', 'props', 'scale', 'difficulty', 'product_type'])
    found_categories = set(stats['category_stats'].keys())
    stats['missing_categories'] = list(all_categories - found_categories)
    
    # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç
    if start_time:
        total_time = time.time() - start_time
        stats['avg_time_per_item'] = total_time / stats['total_items'] if stats['total_items'] > 0 else 0
    
    return stats

def validate_classification_results(df: pd.DataFrame, cache: dict, threshold: float = 0.3) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–±–ª–µ–º."""
    warnings = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
    empty_desc_count = df['description'].isna().sum() + (df['description'] == '').sum()
    if empty_desc_count > 0:
        warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {empty_desc_count} –∑–∞–ø–∏—Å–µ–π —Å –ø—É—Å—Ç—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicate_desc = df[df['description'].duplicated()]['description'].count()
    if duplicate_desc > 0:
        warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {duplicate_desc} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ–ø–∏—Å–∞–Ω–∏–π")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    for _, row in df.iterrows():
        desc = str(row.get("description", "")).strip()
        if not desc or desc.lower() == 'nan':
            continue
            
        h = md5(desc.encode()).hexdigest()
        if h in cache:
            cache_data = cache[h]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            empty_categories = [cat for cat, tags in cache_data.items() if not tags]
            if empty_categories:
                warnings.append(f"–ó–∞–ø–∏—Å—å '{row.get('name', 'Unknown')}' –Ω–µ –∏–º–µ–µ—Ç —Ç–µ–≥–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö: {', '.join(empty_categories)}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∏–∑–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            low_confidence_tags = []
            for cat, tags in cache_data.items():
                low_conf = [tag for tag, conf in tags.items() if conf < threshold]
                if low_conf:
                    low_confidence_tags.extend(low_conf)
            
            if low_confidence_tags:
                warnings.append(f"–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ({threshold}) –¥–ª—è —Ç–µ–≥–æ–≤ {', '.join(low_confidence_tags)} –≤ –∑–∞–ø–∏—Å–∏ '{row.get('name', 'Unknown')}'")
    
    return warnings

def export_results(df: pd.DataFrame, results: List[dict], all_json: List[str], base_output_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö."""
    # CSV —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    result_df = pd.DataFrame(results)
    result_df['tags_json'] = all_json
    result_df.to_csv(base_output_path, index=False)
    logger.info(f"–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {base_output_path}")
    
    # JSON —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    base, _ = os.path.splitext(base_output_path)
    json_path = f"{base}_detailed.json"
    detailed_results = []
    
    for i, (_, row) in enumerate(df.iterrows()):
        result = {
            'name': row.get('name', ''),
            'description': row.get('description', ''),
            'tags': json.loads(all_json[i]) if i < len(all_json) else {},
            'metadata': {
                'source': row.get('source', ''),
                'url': row.get('url', ''),
                'date_added': row.get('date_added', '')
            }
        }
        detailed_results.append(result)
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, ensure_ascii=False, indent=2)
    logger.info(f"–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {json_path}")
    
    # Excel —Å –≤–∫–ª–∞–¥–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    excel_path = f"{base}_categories.xlsx"
    with pd.ExcelWriter(excel_path) as writer:
        # –û—Å–Ω–æ–≤–Ω–æ–π –ª–∏—Å—Ç
        result_df.to_excel(writer, sheet_name='All Results', index=False)
        
        # –õ–∏—Å—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = ['effect', 'props', 'scale', 'difficulty', 'product_type']
        for cat in categories:
            cat_data = []
            for i, row in df.iterrows():
                if i >= len(all_json):
                    continue
                tags = json.loads(all_json[i])
                if cat in tags:
                    cat_tags = tags[cat]
                    if cat_tags:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–≥–∏ –≤ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        cat_data.append({
                            'name': row.get('name', ''),
                            'description': row.get('description', ''),
                            'tags': ', '.join([f"{tag} ({conf:.2f})" for tag, conf in cat_tags.items()]),
                            'tags_ru': ', '.join([f"{LABEL_TRANSLATIONS.get(tag, tag)} ({conf:.2f})" 
                                                for tag, conf in cat_tags.items()])
                        })
            
            if cat_data:  # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                cat_df = pd.DataFrame(cat_data)
                cat_df.to_excel(writer, sheet_name=cat, index=False)
    
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {excel_path}")
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    txt_path = f"{base}_report.txt"
    with open(txt_path, 'w', encoding='utf-8') as f:
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        f.write("=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ===\n\n")
        f.write(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df)} –∑–∞–ø–∏—Å–µ–π\n")
        f.write(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π: {df['description'].nunique()}\n\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        f.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n")
        for cat in categories:
            cat_count = sum(1 for j in all_json if cat in json.loads(j))
            f.write(f"{cat}: {cat_count} –∑–∞–ø–∏—Å–µ–π\n")
        
        # –¢–æ–ø —Ç–µ–≥–æ–≤
        f.write("\n–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–µ–≥–∏:\n")
        tag_counts = defaultdict(int)
        for j in all_json:
            tags_dict = json.loads(j)
            for cat_tags in tags_dict.values():
                for tag in cat_tags:
                    tag_counts[tag] += 1
        
        for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
            f.write(f"{tag} ({LABEL_TRANSLATIONS.get(tag, tag)}): {count}\n")
    
    logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {txt_path}")

def clean_and_preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """–û—á–∏—â–∞–µ—Ç –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π."""
    logger.info("–ù–∞—á–∞–ª–æ –æ—á–∏—Å—Ç–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é DataFrame
    df_clean = df.copy()
    
    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    initial_rows = len(df_clean)
    df_clean = df_clean.dropna(subset=['description'])
    dropped_rows = initial_rows - len(df_clean)
    if dropped_rows > 0:
        logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏")
    
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π
    def clean_text(text):
        if pd.isna(text):
            return ""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–µ
        text = str(text)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = ' '.join(text.split())
        # –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s\-.,!?]', ' ', text)
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'([.,!?])\1+', r'\1', text)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        text = ' '.join(text.split())
        return text
    
    df_clean['description'] = df_clean['description'].apply(clean_text)
    df_clean['name'] = df_clean['name'].apply(clean_text)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    initial_rows = len(df_clean)
    df_clean = df_clean.drop_duplicates(subset=['description'])
    dropped_rows = initial_rows - len(df_clean)
    if dropped_rows > 0:
        logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ–ø–∏—Å–∞–Ω–∏–π")
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ –æ–ø–∏—Å–∞–Ω–∏—è
    min_desc_length = 10  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    initial_rows = len(df_clean)
    df_clean = df_clean[df_clean['description'].str.len() >= min_desc_length]
    dropped_rows = initial_rows - len(df_clean)
    if dropped_rows > 0:
        logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} —Å—Ç—Ä–æ–∫ —Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ (< {min_desc_length} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df_clean['description_length'] = df_clean['description'].str.len()
    df_clean['name_length'] = df_clean['name'].str.len()
    df_clean['word_count'] = df_clean['description'].str.split().str.len()
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –æ–ø–∏—Å–∞–Ω–∏—è (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
    df_clean = df_clean.sort_values('description_length', ascending=False)
    
    logger.info(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df_clean)}")
    
    return df_clean

def tag_csv_with_progress(input_file, output_file, limit=None, threshold=0.3, top_k=3, batch_size=8, cache_file='cache.pkl', force=False, rules_priority='append'):
    try:
        start_time = time.time()
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä limit: {limit}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(input_file)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {input_file}, –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {len(df)}")
        df = clean_and_preprocess_data(df)
        
        if limit:
            df = df.head(limit)
            logger.info(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {limit} —Å—Ç—Ä–æ–∫")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–ª–∞–≥ force
        cache = {}
        if os.path.exists(cache_file) and not force:
            try:
                with open(cache_file, 'rb') as f:
                    cache = pickle.load(f)
                logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∫–µ—à –∏–∑ {cache_file}, –∑–∞–ø–∏—Å–µ–π: {len(cache)}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–µ—à–∞: {str(e)}")
                cache = {}

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞...")
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-mnli")
        model = AutoModelForSequenceClassification.from_pretrained("facebook/bart-large-mnli")
        classifier = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1, batch_size=batch_size)
        logger.info("–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        categories = [
            ("effect", effects_eng),
            ("props", props_eng),
            ("scale", scale_eng),
            ("difficulty", difficulty_eng),
            ("product_type", product_type_eng)
        ]

        results = []
        all_json = []
        progress = tqdm(total=len(df), desc="Classifying", dynamic_ncols=True)

        # –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–Ω–µ—Ç –≤ –∫–µ—à–µ)
        to_process = []
        for i, row in df.iterrows():
            if limit and i >= limit:
                break
            desc = str(row.get("description", "")).strip()
            name = str(row.get("name", "")).strip()
            if not desc or desc.lower() == 'nan':
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ {i}: –ø—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
                continue
            h = md5(desc.encode()).hexdigest()
            if h not in cache:
                to_process.append((i, desc, name))
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(to_process)} —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–µ –≤ –∫–µ—à–µ)")

        # –ë–∞—Ç—á–∏–Ω–≥ –æ–ø–∏—Å–∞–Ω–∏–π
        for start in range(0, len(to_process), batch_size):
            batch = to_process[start:start+batch_size]
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {start//batch_size + 1}, —Ä–∞–∑–º–µ—Ä: {len(batch)}")
            
            batch_indices = [i for i, _, _ in batch]
            batch_descs = [desc for _, desc, _ in batch]
            batch_names = [name for _, _, name in batch]
            batch_results = {}
            
            # –ü–æ–ª—É—á–∞–µ–º rule-based —Ç–µ–≥–∏
            rule_tags = [rule_based_tags(name, desc) for name, desc in zip(batch_names, batch_descs)]
            logger.debug(f"–ü–æ–ª—É—á–µ–Ω—ã rule-based —Ç–µ–≥–∏ –¥–ª—è –±–∞—Ç—á–∞: {rule_tags}")
            
            for cat, labels in categories:
                cat_results = batch_infer(classifier, batch_descs, labels)
                for idx, res, rule_tag in zip(batch_indices, cat_results, rule_tags):
                    desc_val = str(df.loc[idx, "description"]).strip()
                    if not desc_val or desc_val.lower() == 'nan':
                        continue
                    h = md5(desc_val.encode()).hexdigest()
                    if h not in cache:
                        cache[h] = {}
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                    if rules_priority == 'prefer_rules' and rule_tag.get(cat):
                        cache[h][cat] = {tag: 1.0 for tag in rule_tag[cat]}
                    elif rules_priority == 'prefer_model':
                        cache[h][cat] = dict(zip(res["labels"], res["scores"]))
                    else:  # append
                        model_scores = dict(zip(res["labels"], res["scores"]))
                        rule_scores = {tag: 1.0 for tag in rule_tag.get(cat, [])}
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–≥–∏, –æ—Ç–¥–∞–≤–∞—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º
                        combined = {**model_scores, **rule_scores}
                        cache[h][cat] = combined
                        
            progress.update(len(batch))

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        tags_per_category = {cat: 0 for cat, _ in categories}
        cache_hits = 0
        for i, row in df.iterrows():
            desc = str(row.get("description", "")).strip()
            name = str(row.get("name", "")).strip()
            if not desc or desc.lower() == 'nan':
                tag_json = {}
                all_json.append(json.dumps(tag_json, ensure_ascii=False))
                tags = {cat: [] for cat, _ in categories}
                tags_ru = {cat: [] for cat, _ in categories}
                results.append({**tags, **{cat+"_ru": tags_ru[cat] for cat in tags_ru}})
                continue
            h = md5(desc.encode()).hexdigest()
            tag_json = cache.get(h, {})
            if h in cache:
                cache_hits += 1
            all_json.append(json.dumps(tag_json, ensure_ascii=False))
            tags = {cat: [] for cat, _ in categories}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            tags_ru = {cat: [] for cat, _ in categories}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            for cat, labels in categories:
                tag_scores = tag_json.get(cat, {})
                filtered = [label for label, score in sorted(tag_scores.items(), key=lambda x: -x[1]) if score >= threshold]
                if top_k:
                    filtered = filtered[:top_k]
                tags[cat] = filtered
                tags_ru[cat] = [LABEL_TRANSLATIONS.get(label, label) for label in filtered]
                tags_per_category[cat] += len(filtered)
            results.append({**tags, **{cat+"_ru": tags_ru[cat] for cat in tags_ru}})
        progress.close()
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
        for cat, _ in categories:
            df[cat] = [r[cat] for r in results]
            df[cat+"_ru"] = [r[cat+"_ru"] for r in results]
        df["tags_json"] = all_json
        df.to_csv(output_file, index=False)
        print(f"‚úî Done! Saved to: {output_file}")
        save_cache(cache, cache_file)
        print(f"–ö–µ—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {cache_file}")
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        total_rows = len(df)
        print("\nüìä –û—Ç—á—ë—Ç:")
        print(f"‚úî –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_rows} —Å—Ç—Ä–æ–∫")
        for cat in tags_per_category:
            avg = tags_per_category[cat] / total_rows if total_rows else 0
            print(f"   {cat:<12} ‚Äî –≤ —Å—Ä–µ–¥–Ω–µ–º {avg:.1f} —Ç–µ–≥–∞(–æ–≤)/—Å—Ç—Ä–æ–∫—É")
        print(f"‚úî –ö–µ—à-—Ö–∏—Ç–æ–≤: {cache_hits} –∏–∑ {total_rows}")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        warnings = validate_classification_results(df, cache, threshold)
        if warnings:
            logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
            for warning in warnings:
                logger.warning(f"- {warning}")
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = collect_statistics(df, cache, start_time)
        stats['warnings'].extend(warnings)  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        report = generate_report(stats)
        logger.info("\n" + report)
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        export_results(df, results, all_json, output_file)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à
        save_cache(cache, cache_file)
        logger.info(f"–ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {cache_file}")
        
        return True, "Success"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        return False, str(e)

if __name__ == "__main__":
    args = get_args()
    tag_csv_with_progress(args.input, args.output, args.limit, args.threshold, args.top_k, args.batch_size, args.cache, args.force, args.rules_priority) 